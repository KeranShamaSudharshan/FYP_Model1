{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Real-Time Clustering Model\n",
    "\n",
    "This notebook prepares training data for the real-time student engagement clustering system.\n",
    "\n",
    "**Purpose**: Create labeled training data where cluster assignments are generated dynamically based on cumulative student performance.\n",
    "\n",
    "**Workflow**:\n",
    "1. Load datasets from Google Drive\n",
    "2. Filter participating students ONLY\n",
    "3. Separate initial questions from regular questions\n",
    "4. Apply dynamic cluster assignment logic\n",
    "5. Create features and labels\n",
    "6. Save preprocessed data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df = pd.read_csv('/content/drive/MyDrive/FYP_Data/Merge_Enhanced_Fixed.csv')\n",
    "participant_df = pd.read_csv('/content/drive/MyDrive/FYP_Data/Participant_Tracking.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} records\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nParticipant tracking: {participant_df.shape[0]} events\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Filter Participating Students ONLY\n",
    "\n",
    "**Critical**: Only students who joined the session should be included in clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get students who joined sessions (participating students)\n",
    "participated_students = participant_df[\n",
    "    participant_df['Event Type'] == 'Joined'\n",
    "]['Admission No'].unique()\n",
    "\n",
    "print(f\"Total students in dataset: {df['Admission No'].nunique()}\")\n",
    "print(f\"Students who participated: {len(participated_students)}\")\n",
    "\n",
    "# Filter dataset to only include participants\n",
    "df_filtered = df[df['Admission No'].isin(participated_students)].copy()\n",
    "\n",
    "print(f\"\\nRecords after filtering: {df_filtered.shape[0]}\")\n",
    "print(f\"Students after filtering: {df_filtered['Admission No'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Separate Initial Questions from Regular Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate initial questions (Quiz# = 0) from regular questions\n",
    "df_initial = df_filtered[df_filtered['Quiz#'] == 0].copy()\n",
    "df_regular = df_filtered[df_filtered['Quiz#'] > 0].copy()\n",
    "\n",
    "print(f\"Initial questions: {df_initial.shape[0]} (should be {len(participated_students)})\")\n",
    "print(f\"Regular questions: {df_regular.shape[0]}\")\n",
    "\n",
    "# Verify each student has exactly 1 initial question\n",
    "initial_counts = df_initial.groupby('Admission No').size()\n",
    "print(f\"\\nStudents with != 1 initial question: {(initial_counts != 1).sum()}\")\n",
    "if (initial_counts != 1).sum() > 0:\n",
    "    print(\"WARNING: Some students have != 1 initial question!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply Dynamic Cluster Assignment Logic\n",
    "\n",
    "For each student, calculate cumulative metrics after each question and assign cluster based on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_cluster(accuracy, avg_response_time, has_network_issue):\n",
    "    \"\"\"\n",
    "    Assign cluster based on cumulative performance metrics.\n",
    "    \n",
    "    Rules:\n",
    "    - If network issue: Passive (can't judge performance fairly)\n",
    "    - High accuracy + fast response: Active\n",
    "    - Medium accuracy + medium response: Moderate\n",
    "    - Otherwise: Passive\n",
    "    \"\"\"\n",
    "    if has_network_issue:\n",
    "        return 'Passive'\n",
    "    \n",
    "    if accuracy > 0.80 and avg_response_time < 30:\n",
    "        return 'Active'\n",
    "    elif accuracy > 0.50 and avg_response_time < 60:\n",
    "        return 'Moderate'\n",
    "    else:\n",
    "        return 'Passive'\n",
    "\n",
    "# Process each student's questions chronologically\n",
    "df_regular_sorted = df_regular.sort_values(['Admission No', 'Timestamp']).copy()\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "training_data = []\n",
    "\n",
    "for student_id in participated_students:\n",
    "    student_questions = df_regular_sorted[df_regular_sorted['Admission No'] == student_id]\n",
    "    \n",
    "    # Initialize cumulative metrics\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    response_times = []\n",
    "    \n",
    "    for idx, row in student_questions.iterrows():\n",
    "        # Update cumulative metrics\n",
    "        if row['Attempt Status'] == 'Completed':\n",
    "            correct_count += row['Is_Correct']\n",
    "            total_count += 1\n",
    "            response_times.append(row['Response Time (seconds)'])\n",
    "            has_network_issue = False\n",
    "        else:\n",
    "            # Not completed - check network\n",
    "            total_count += 1\n",
    "            response_times.append(row['Response Time (seconds)'])\n",
    "            has_network_issue = (row['RTT (ms)'] > 3000 or row['Jitter (ms)'] > 2000 or row['Stability (%)'] < 75)\n",
    "        \n",
    "        # Calculate cumulative metrics\n",
    "        if total_count > 0:\n",
    "            cumulative_accuracy = correct_count / total_count\n",
    "            avg_response_time = np.mean(response_times)\n",
    "            \n",
    "            # Assign cluster\n",
    "            cluster = assign_cluster(cumulative_accuracy, avg_response_time, has_network_issue)\n",
    "            \n",
    "            # Create feature vector\n",
    "            if row['Attempt Status'] == 'Completed':\n",
    "                # For completed: Response time + correctness ONLY\n",
    "                features = {\n",
    "                    'cumulative_accuracy': cumulative_accuracy,\n",
    "                    'avg_response_time': avg_response_time,\n",
    "                    'total_questions': total_count,\n",
    "                    'current_response_time': row['Response Time (seconds)'],\n",
    "                    'is_correct': row['Is_Correct'],\n",
    "                    'cluster': cluster,\n",
    "                    'admission_no': student_id,\n",
    "                    'question_type': 'completed'\n",
    "                }\n",
    "            else:\n",
    "                # For not completed: Include network metrics\n",
    "                features = {\n",
    "                    'cumulative_accuracy': cumulative_accuracy,\n",
    "                    'avg_response_time': avg_response_time,\n",
    "                    'total_questions': total_count,\n",
    "                    'current_response_time': row['Response Time (seconds)'],\n",
    "                    'rtt': row['RTT (ms)'],\n",
    "                    'jitter': row['Jitter (ms)'],\n",
    "                    'stability': row['Stability (%)'],\n",
    "                    'cluster': cluster,\n",
    "                    'admission_no': student_id,\n",
    "                    'question_type': 'not_completed'\n",
    "                }\n",
    "            \n",
    "            training_data.append(features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "training_df = pd.DataFrame(training_data)\n",
    "\n",
    "print(f\"Training samples created: {len(training_df)}\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "print(training_df['cluster'].value_counts())\n",
    "print(f\"\\nCluster distribution (%):\")\n",
    "print(training_df['cluster'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Initial Question Features\n",
    "\n",
    "Initial questions use different features (response time + network metrics) for K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare initial question features\n",
    "initial_features = df_initial[[\n",
    "    'Response Time (seconds)',\n",
    "    'RTT (ms)',\n",
    "    'Jitter (ms)',\n",
    "    'Stability (%)'\n",
    "]].values\n",
    "\n",
    "# Assign initial clusters based on response time and network\n",
    "# Simple rule: Good network + fast response = Moderate start, else Passive\n",
    "initial_clusters = []\n",
    "for _, row in df_initial.iterrows():\n",
    "    if row['Response Time (seconds)'] < 45 and row['Stability (%)'] > 80:\n",
    "        initial_clusters.append('Moderate')\n",
    "    else:\n",
    "        initial_clusters.append('Passive')\n",
    "\n",
    "df_initial['initial_cluster'] = initial_clusters\n",
    "\n",
    "print(f\"Initial question features shape: {initial_features.shape}\")\n",
    "print(f\"\\nInitial cluster distribution:\")\n",
    "print(df_initial['initial_cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Feature Matrices for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate completed and not completed questions\n",
    "completed_df = training_df[training_df['question_type'] == 'completed'].copy()\n",
    "not_completed_df = training_df[training_df['question_type'] == 'not_completed'].copy()\n",
    "\n",
    "print(f\"Completed questions: {len(completed_df)}\")\n",
    "print(f\"Not completed questions: {len(not_completed_df)}\")\n",
    "\n",
    "# Features for completed questions (NO network metrics)\n",
    "X_completed = completed_df[[\n",
    "    'cumulative_accuracy',\n",
    "    'avg_response_time',\n",
    "    'total_questions',\n",
    "    'current_response_time',\n",
    "    'is_correct'\n",
    "]].values\n",
    "\n",
    "y_completed = completed_df['cluster'].values\n",
    "\n",
    "# Features for not completed questions (WITH network metrics)\n",
    "X_not_completed = not_completed_df[[\n",
    "    'cumulative_accuracy',\n",
    "    'avg_response_time',\n",
    "    'total_questions',\n",
    "    'current_response_time',\n",
    "    'rtt',\n",
    "    'jitter',\n",
    "    'stability'\n",
    "]].values\n",
    "\n",
    "y_not_completed = not_completed_df['cluster'].values\n",
    "\n",
    "print(f\"\\nCompleted features shape: {X_completed.shape}\")\n",
    "print(f\"Not completed features shape: {X_not_completed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale initial question features\n",
    "scaler_initial = StandardScaler()\n",
    "X_initial_scaled = scaler_initial.fit_transform(initial_features)\n",
    "\n",
    "# Scale completed question features\n",
    "scaler_completed = StandardScaler()\n",
    "X_completed_scaled = scaler_completed.fit_transform(X_completed)\n",
    "\n",
    "# Scale not completed question features\n",
    "scaler_not_completed = StandardScaler()\n",
    "X_not_completed_scaled = scaler_not_completed.fit_transform(X_not_completed)\n",
    "\n",
    "print(\"Feature scaling completed.\")\n",
    "print(f\"\\nInitial features - Mean: {X_initial_scaled.mean():.4f}, Std: {X_initial_scaled.std():.4f}\")\n",
    "print(f\"Completed features - Mean: {X_completed_scaled.mean():.4f}, Std: {X_completed_scaled.std():.4f}\")\n",
    "print(f\"Not completed features - Mean: {X_not_completed_scaled.mean():.4f}, Std: {X_not_completed_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "import os\n",
    "output_dir = '/content/drive/MyDrive/FYP_Data/Preprocessed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save arrays\n",
    "np.save(f'{output_dir}/X_initial_scaled.npy', X_initial_scaled)\n",
    "np.save(f'{output_dir}/y_initial.npy', df_initial['initial_cluster'].values)\n",
    "\n",
    "np.save(f'{output_dir}/X_completed_scaled.npy', X_completed_scaled)\n",
    "np.save(f'{output_dir}/y_completed.npy', y_completed)\n",
    "\n",
    "np.save(f'{output_dir}/X_not_completed_scaled.npy', X_not_completed_scaled)\n",
    "np.save(f'{output_dir}/y_not_completed.npy', y_not_completed)\n",
    "\n",
    "# Save scalers\n",
    "with open(f'{output_dir}/scaler_initial.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_initial, f)\n",
    "\n",
    "with open(f'{output_dir}/scaler_completed.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_completed, f)\n",
    "\n",
    "with open(f'{output_dir}/scaler_not_completed.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_not_completed, f)\n",
    "\n",
    "# Save full training dataset as CSV\n",
    "training_df.to_csv(f'{output_dir}/Final_Training_Data.csv', index=False)\n",
    "\n",
    "print(\"\\n✅ All preprocessed data saved successfully!\")\n",
    "print(f\"\\nSaved files:\")\n",
    "print(f\"  - X_initial_scaled.npy ({X_initial_scaled.shape})\")\n",
    "print(f\"  - y_initial.npy ({len(df_initial)} labels)\")\n",
    "print(f\"  - X_completed_scaled.npy ({X_completed_scaled.shape})\")\n",
    "print(f\"  - y_completed.npy ({len(y_completed)} labels)\")\n",
    "print(f\"  - X_not_completed_scaled.npy ({X_not_completed_scaled.shape})\")\n",
    "print(f\"  - y_not_completed.npy ({len(y_not_completed)} labels)\")\n",
    "print(f\"  - scaler_initial.pkl\")\n",
    "print(f\"  - scaler_completed.pkl\")\n",
    "print(f\"  - scaler_not_completed.pkl\")\n",
    "print(f\"  - Final_Training_Data.csv ({len(training_df)} records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Preprocessing Complete!**\n",
    "\n",
    "- ✅ Filtered participating students only\n",
    "- ✅ Applied dynamic cluster assignment based on cumulative performance\n",
    "- ✅ Created separate features for:\n",
    "  - Initial questions (for K-Means baseline)\n",
    "  - Completed questions (response time + correctness)\n",
    "  - Not completed questions (+ network metrics)\n",
    "- ✅ Scaled all features\n",
    "- ✅ Saved 10 files for model training\n",
    "\n",
    "**Next Step**: Open `02_Model_Training_RealTime.ipynb` to train the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
