{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing - Student Engagement Analytics\n",
    "\n",
    "**Objective**: Load dataset from Google Drive, clean, and prepare features for Model 1\n",
    "\n",
    "**Steps**:\n",
    "1. Mount Google Drive\n",
    "2. Load dataset (Merge.csv)\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Data Cleaning\n",
    "5. Feature Engineering\n",
    "6. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn -q\n",
    "\n",
    "print(\"âœ… Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\nâœ… Google Drive mounted successfully\")\n",
    "print(\"\\nPlease upload 'Merge.csv' to your Google Drive\")\n",
    "print(\"Suggested path: /content/drive/MyDrive/FYP_Data/Merge.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Google Drive\n",
    "# MODIFY THIS PATH TO MATCH YOUR GOOGLE DRIVE LOCATION\n",
    "DATA_PATH = '/content/drive/MyDrive/FYP_Data/Merge.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(\"âœ… Dataset loaded successfully!\")\n",
    "    print(f\"\\nDataset Shape: {df.shape}\")\n",
    "    print(f\"Total Records: {len(df)}\")\n",
    "    print(f\"Total Features: {len(df.columns)}\")\nexcept FileNotFoundError:\n",
    "    print(\"âŒ File not found! Please check the path and upload Merge.csv to Google Drive\")\n",
    "    print(f\"Looking for file at: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\"*80)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(\"Missing Values:\")\n",
    "print(\"=\"*80)\n",
    "missing = df.isnull().sum()\n",
    "missing_percent = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "if missing_df['Missing_Count'].sum() == 0:\n",
    "    print(\"âœ… No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "print(\"Target Variable: Engagement Level\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nDistribution:\")\n",
    "print(df['Engagement Level'].value_counts())\n",
    "print(\"\\nPercentage:\")\n",
    "print(df['Engagement Level'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['Engagement Level'].value_counts().plot(kind='bar', color=['#6BCB77', '#FFD93D', '#FF6B6B'])\n",
    "plt.title('Engagement Level Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Engagement Level', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâš ï¸ Note: Dataset is imbalanced (Active: 63%, Passive: 31.5%, Moderate: 5.5%)\")\n",
    "print(\"We'll handle this with class weights in the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key features\n",
    "print(\"Key Feature Analysis:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Student statistics\n",
    "print(f\"\\nTotal Students: {df['Student Name'].nunique()}\")\n",
    "print(f\"Total Quizzes: {df['Quiz#'].nunique()}\")\n",
    "print(f\"Total Questions: {df['Question ID'].nunique()}\")\n",
    "\n",
    "# Network quality distribution\n",
    "print(\"\\nNetwork Quality Distribution:\")\n",
    "print(df['Network Quality'].value_counts())\n",
    "\n",
    "# Correctness statistics\n",
    "print(\"\\nAnswer Correctness:\")\n",
    "print(df['Is Correct'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key numerical features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Distribution of Key Features', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Response Time\n",
    "axes[0, 0].hist(df['Response Time (sec)'], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Response Time (sec)')\n",
    "axes[0, 0].set_xlabel('Time (seconds)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# RTT\n",
    "axes[0, 1].hist(df['RTT (ms)'], bins=50, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('RTT (ms)')\n",
    "axes[0, 1].set_xlabel('RTT (ms)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Jitter\n",
    "axes[0, 2].hist(df['Jitter (ms)'], bins=50, color='lightcoral', edgecolor='black')\n",
    "axes[0, 2].set_title('Jitter (ms)')\n",
    "axes[0, 2].set_xlabel('Jitter (ms)')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "# Stability\n",
    "axes[1, 0].hist(df['Stability (%)'], bins=50, color='gold', edgecolor='black')\n",
    "axes[1, 0].set_title('Stability (%)')\n",
    "axes[1, 0].set_xlabel('Stability (%)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Engagement by Network Quality\n",
    "pd.crosstab(df['Network Quality'], df['Engagement Level']).plot(kind='bar', ax=axes[1, 1], color=['#6BCB77', '#FFD93D', '#FF6B6B'])\n",
    "axes[1, 1].set_title('Engagement by Network Quality')\n",
    "axes[1, 1].set_xlabel('Network Quality')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide last subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"Starting Data Cleaning...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Handle any missing values (if present)\n",
    "print(\"\\n1. Checking for missing values...\")\n",
    "if df_clean.isnull().sum().sum() > 0:\n",
    "    print(\"   Found missing values. Filling...\")\n",
    "    # Fill numeric columns with median\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].median())\n",
    "    \n",
    "    # Fill categorical with mode\n",
    "    categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "    print(\"   âœ… Missing values handled\")\n",
    "else:\n",
    "    print(\"   âœ… No missing values found\")\n",
    "\n",
    "# 2. Remove duplicates\n",
    "print(\"\\n2. Checking for duplicates...\")\n",
    "duplicates = df_clean.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"   Found {duplicates} duplicate rows. Removing...\")\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    print(\"   âœ… Duplicates removed\")\n",
    "else:\n",
    "    print(\"   âœ… No duplicates found\")\n",
    "\n",
    "# 3. Handle outliers (optional - using IQR method for key features)\n",
    "print(\"\\n3. Analyzing outliers in Response Time...\")\n",
    "Q1 = df_clean['Response Time (sec)'].quantile(0.25)\n",
    "Q3 = df_clean['Response Time (sec)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((df_clean['Response Time (sec)'] < (Q1 - 1.5 * IQR)) | \n",
    "            (df_clean['Response Time (sec)'] > (Q3 + 1.5 * IQR))).sum()\n",
    "print(f\"   Found {outliers} outliers in Response Time\")\n",
    "print(f\"   Keeping outliers as they represent real behavioral patterns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ… Data Cleaning Complete\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Cleaned shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Engineering...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Convert 'Is Correct' to binary\n",
    "print(\"\\n1. Converting 'Is Correct' to binary...\")\n",
    "df_clean['Is_Correct_Binary'] = df_clean['Is Correct'].apply(\n",
    "    lambda x: 1 if str(x).lower() == 'yes' else 0\n",
    ")\n",
    "print(\"   âœ… Done\")\n",
    "\n",
    "# 2. Encode Engagement Level (target)\n",
    "print(\"\\n2. Encoding Engagement Level...\")\n",
    "engagement_mapping = {'Passive': 0, 'Moderate': 1, 'Active': 2}\n",
    "df_clean['Engagement_Encoded'] = df_clean['Engagement Level'].map(engagement_mapping)\n",
    "print(\"   Mapping: Passive=0, Moderate=1, Active=2\")\n",
    "print(\"   âœ… Done\")\n",
    "\n",
    "# 3. Encode Network Quality\n",
    "print(\"\\n3. Encoding Network Quality...\")\n",
    "network_mapping = {'Poor': 0, 'Fair': 1, 'Good': 2, 'Excellent': 3}\n",
    "df_clean['Network_Quality_Encoded'] = df_clean['Network Quality'].map(network_mapping)\n",
    "# Handle any unmapped values\n",
    "if df_clean['Network_Quality_Encoded'].isnull().any():\n",
    "    print(\"   Warning: Some network quality values not mapped. Filling with 1 (Fair)\")\n",
    "    df_clean['Network_Quality_Encoded'].fillna(1, inplace=True)\n",
    "print(\"   âœ… Done\")\n",
    "\n",
    "# 4. Calculate student-level aggregated features\n",
    "print(\"\\n4. Creating student-level aggregated features...\")\n",
    "\n",
    "# Average accuracy per student\n",
    "student_accuracy = df_clean.groupby('Student Name')['Is_Correct_Binary'].mean().reset_index()\n",
    "student_accuracy.columns = ['Student Name', 'Student_Avg_Accuracy']\n",
    "df_clean = df_clean.merge(student_accuracy, on='Student Name', how='left')\n",
    "\n",
    "# Average response time per student\n",
    "student_response_time = df_clean.groupby('Student Name')['Response Time (sec)'].mean().reset_index()\n",
    "student_response_time.columns = ['Student Name', 'Student_Avg_Response_Time']\n",
    "df_clean = df_clean.merge(student_response_time, on='Student Name', how='left')\n",
    "\n",
    "# Average RTT per student\n",
    "student_rtt = df_clean.groupby('Student Name')['RTT (ms)'].mean().reset_index()\n",
    "student_rtt.columns = ['Student Name', 'Student_Avg_RTT']\n",
    "df_clean = df_clean.merge(student_rtt, on='Student Name', how='left')\n",
    "\n",
    "print(\"   âœ… Done\")\n",
    "\n",
    "# 5. Create time-based features\n",
    "print(\"\\n5. Creating time-based features...\")\n",
    "df_clean['Answered_At_Datetime'] = pd.to_datetime(df_clean['Answered At'])\n",
    "df_clean['Hour_of_Day'] = df_clean['Answered_At_Datetime'].dt.hour\n",
    "df_clean['Day_of_Week'] = df_clean['Answered_At_Datetime'].dt.dayofweek\n",
    "print(\"   âœ… Done\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Feature Engineering Complete\")\n",
    "print(f\"Total features now: {len(df_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display new features\n",
    "print(\"New Features Created:\")\n",
    "print(\"=\"*80)\n",
    "new_features = ['Is_Correct_Binary', 'Engagement_Encoded', 'Network_Quality_Encoded',\n",
    "                'Student_Avg_Accuracy', 'Student_Avg_Response_Time', 'Student_Avg_RTT',\n",
    "                'Hour_of_Day', 'Day_of_Week']\n",
    "df_clean[new_features].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selecting Features for Models...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Features for Model 1 (Clustering + Prediction)\n",
    "feature_columns = [\n",
    "    # Response metrics\n",
    "    'Response Time (sec)',\n",
    "    'Response Time (ms)',\n",
    "    \n",
    "    # Network metrics\n",
    "    'RTT (ms)',\n",
    "    'Jitter (ms)',\n",
    "    'Stability (%)',\n",
    "    'Network_Quality_Encoded',\n",
    "    \n",
    "    # Performance metrics\n",
    "    'Is_Correct_Binary',\n",
    "    'Student_Avg_Accuracy',\n",
    "    'Student_Avg_Response_Time',\n",
    "    'Student_Avg_RTT',\n",
    "    \n",
    "    # Additional features\n",
    "    'Hour_of_Day',\n",
    "    'Day_of_Week'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target_column = 'Engagement_Encoded'\n",
    "\n",
    "print(f\"\\nSelected {len(feature_columns)} features for modeling:\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(f\"\\nTarget variable: {target_column} (Engagement Level)\")\n",
    "print(\"\\nâœ… Feature selection complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"Correlation Analysis with Target Variable:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = df_clean[feature_columns + [target_column]].corr()[target_column].sort_values(ascending=False)\n",
    "print(correlations[1:])  # Exclude self-correlation\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df_clean[feature_columns + [target_column]].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f', square=True)\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data to Google Drive\n",
    "OUTPUT_PATH = '/content/drive/MyDrive/FYP_Data/'\n",
    "\n",
    "print(\"Saving Preprocessed Data...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Save full preprocessed dataset\n",
    "full_output_path = OUTPUT_PATH + 'Merge_Preprocessed.csv'\n",
    "df_clean.to_csv(full_output_path, index=False)\n",
    "print(f\"âœ… Full preprocessed data saved: {full_output_path}\")\n",
    "\n",
    "# 2. Save feature matrix and target\n",
    "X = df_clean[feature_columns]\n",
    "y = df_clean[target_column]\n",
    "\n",
    "features_output_path = OUTPUT_PATH + 'Features_X.csv'\n",
    "target_output_path = OUTPUT_PATH + 'Target_y.csv'\n",
    "\n",
    "X.to_csv(features_output_path, index=False)\n",
    "y.to_csv(target_output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Feature matrix saved: {features_output_path}\")\n",
    "print(f\"âœ… Target variable saved: {target_output_path}\")\n",
    "\n",
    "# 3. Save student-level aggregated data\n",
    "student_data = df_clean.groupby('Student Name').agg({\n",
    "    'Student_Avg_Accuracy': 'first',\n",
    "    'Student_Avg_Response_Time': 'first',\n",
    "    'Student_Avg_RTT': 'first',\n",
    "    'RTT (ms)': 'mean',\n",
    "    'Jitter (ms)': 'mean',\n",
    "    'Stability (%)': 'mean',\n",
    "    'Network_Quality_Encoded': 'mean',\n",
    "    'Engagement_Encoded': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "}).reset_index()\n",
    "\n",
    "student_output_path = OUTPUT_PATH + 'Student_Aggregated_Data.csv'\n",
    "student_data.to_csv(student_output_path, index=False)\n",
    "print(f\"âœ… Student-level data saved: {student_output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… All preprocessed data saved successfully!\")\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  - Full dataset: {len(df_clean)} records, {len(df_clean.columns)} features\")\n",
    "print(f\"  - Feature matrix: {X.shape}\")\n",
    "print(f\"  - Target vector: {y.shape}\")\n",
    "print(f\"  - Student-level data: {len(student_data)} students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Summary for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DATA PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š Dataset Overview:\")\n",
    "print(f\"  - Total Records: {len(df_clean)}\")\n",
    "print(f\"  - Total Students: {df_clean['Student Name'].nunique()}\")\n",
    "print(f\"  - Total Features: {len(feature_columns)}\")\n",
    "print(f\"  - Target Classes: {df_clean['Engagement Level'].nunique()}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Target Distribution:\")\n",
    "for level, count in df_clean['Engagement Level'].value_counts().items():\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"  - {level}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nðŸ”§ Features Created:\")\n",
    "print(f\"  - Binary features: 1 (Is_Correct_Binary)\")\n",
    "print(f\"  - Encoded features: 2 (Engagement, Network Quality)\")\n",
    "print(f\"  - Aggregated features: 3 (Student averages)\")\n",
    "print(f\"  - Time features: 2 (Hour, Day of Week)\")\n",
    "\n",
    "print(\"\\nðŸ’¾ Saved Files:\")\n",
    "print(f\"  1. Merge_Preprocessed.csv - Full preprocessed dataset\")\n",
    "print(f\"  2. Features_X.csv - Feature matrix for modeling\")\n",
    "print(f\"  3. Target_y.csv - Target variable\")\n",
    "print(f\"  4. Student_Aggregated_Data.csv - Student-level aggregated data\")\n",
    "\n",
    "print(\"\\nâœ… Ready for Model 1 Training!\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Open 'Model1_Clustering_Prediction.ipynb'\")\n",
    "print(\"  2. Load preprocessed data from Google Drive\")\n",
    "print(\"  3. Train clustering model (unsupervised)\")\n",
    "print(\"  4. Train prediction model (supervised)\")\n",
    "print(\"  5. Evaluate and validate models\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
