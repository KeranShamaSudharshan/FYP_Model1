{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
     "id": "header"
   },
   "source": [
    "# Data Preprocessing - Enhanced Student Engagement Dataset\n",
    "\n",
    "**Purpose**: Load and preprocess the enhanced dataset with initial questions, participant tracking, and completion status\n",
    "\n",
    "**Datasets Used**:\n",
    "- `Merge_Enhanced.csv` - Main dataset with initial questions\n",
    "- `Participant_Tracking.csv` - Real-time participant presence\n",
    "\n",
    "**Key Features**:\n",
    "1. Filter only participating students\n",
    "2. Separate initial questions (Quiz# = 0) from quiz questions\n",
    "3. Apply different feature selection for Completed vs Not Completed\n",
    "4. Cross-validate with participant tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn -q\n",
    "print(\"✅ Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"\\n✅ Drive mounted\")\n",
    "print(\"\\nUpload these files to /content/drive/MyDrive/FYP_Data/:\")\n",
    "print(\"  1. Merge_Enhanced.csv\")\n",
    "print(\"  2. Participant_Tracking.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"✅ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Enhanced Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (modify if needed)\n",
    "DATA_PATH = '/content/drive/MyDrive/FYP_Data/'\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "df_enhanced = pd.read_csv(DATA_PATH + 'Merge_Enhanced.csv')\n",
    "participant_df = pd.read_csv(DATA_PATH + 'Participant_Tracking.csv')\n",
    "\n",
    "print(f\"✅ Enhanced dataset loaded: {df_enhanced.shape}\")\n",
    "print(f\"✅ Participant tracking loaded: {participant_df.shape}\")\n",
    "print(f\"\\nTotal students: {df_enhanced['Admission No'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter Only Participating Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get students who actually participated (joined sessions)\n",
    "participated_students = participant_df[\n",
    "    participant_df['Event Type'] == 'Joined'\n",
    "]['Admission No'].unique()\n",
    "\n",
    "print(f\"Students who participated: {len(participated_students)}\")\n",
    "\n",
    "# Filter dataset to only include participating students\n",
    "df = df_enhanced[df_enhanced['Admission No'].isin(participated_students)].copy()\n",
    "\n",
    "print(f\"✅ Filtered to participating students only\")\n",
    "print(f\"Records: {len(df)} (from {len(df_enhanced)})\")\n",
    "print(f\"Students: {df['Admission No'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Separate Question Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate initial questions from quiz questions\n",
    "initial_questions = df[df['Quiz#'] == 0].copy()\n",
    "quiz_questions = df[df['Quiz#'] > 0].copy()\n",
    "\n",
    "print(\"Dataset Separation:\")\n",
    "print(f\"  Initial Questions (Quiz# = 0): {len(initial_questions)}\")\n",
    "print(f\"  Quiz Questions (Quiz# > 0): {len(quiz_questions)}\")\n",
    "\n",
    "# Further separate by completion status\n",
    "completed = quiz_questions[quiz_questions['Attempt Status'] == 'Completed'].copy()\n",
    "not_completed = quiz_questions[quiz_questions['Attempt Status'] == 'Not Completed'].copy()\n",
    "\n",
    "print(f\"\\nQuiz Questions Breakdown:\")\n",
    "print(f\"  Completed: {len(completed)} ({len(completed)/len(quiz_questions)*100:.1f}%)\")\n",
    "print(f\"  Not Completed: {len(not_completed)} ({len(not_completed)/len(quiz_questions)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features\n",
    "def prepare_features(data_df):\n",
    "    df_prep = data_df.copy()\n",
    "    \n",
    "    # Binary encoding\n",
    "    df_prep['Is_Correct_Binary'] = df_prep['Is Correct'].apply(\n",
    "        lambda x: 1 if str(x).lower() == 'yes' else 0\n",
    "    )\n",
    "    \n",
    "    # Engagement encoding\n",
    "    engagement_map = {'Passive': 0, 'Moderate': 1, 'Active': 2}\n",
    "    df_prep['Engagement_Encoded'] = df_prep['Engagement Level'].map(engagement_map)\n",
    "    \n",
    "    # Network quality encoding\n",
    "    network_map = {'Poor': 0, 'Fair': 1, 'Good': 2, 'Excellent': 3}\n",
    "    df_prep['Network_Quality_Encoded'] = df_prep['Network Quality'].map(network_map)\n",
    "    df_prep['Network_Quality_Encoded'].fillna(1, inplace=True)\n",
    "    \n",
    "    return df_prep\n",
    "\n",
    "# Apply to all datasets\n",
    "initial_questions = prepare_features(initial_questions)\n",
    "completed = prepare_features(completed)\n",
    "not_completed = prepare_features(not_completed)\n",
    "\n",
    "print(\"✅ Feature engineering complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection by Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 1: Initial questions (for baseline clustering)\n",
    "initial_features = [\n",
    "    'Response Time (sec)',\n",
    "    'RTT (ms)',\n",
    "    'Jitter (ms)',\n",
    "    'Stability (%)'\n",
    "]\n",
    "\n",
    "X_initial = initial_questions[initial_features].copy()\n",
    "y_initial = initial_questions['Engagement_Encoded'].copy()\n",
    "\n",
    "print(\"Stage 1 - Initial Questions:\")\n",
    "print(f\"  Features: {initial_features}\")\n",
    "print(f\"  Shape: {X_initial.shape}\")\n",
    "\n",
    "# STAGE 2: Completed questions (NO network params)\n",
    "completed_features = [\n",
    "    'Response Time (sec)',\n",
    "    'Is_Correct_Binary'\n",
    "]\n",
    "\n",
    "X_completed = completed[completed_features].copy()\n",
    "y_completed = completed['Engagement_Encoded'].copy()\n",
    "\n",
    "print(\"\\nStage 2 - Completed Questions:\")\n",
    "print(f\"  Features: {completed_features}\")\n",
    "print(f\"  Shape: {X_completed.shape}\")\n",
    "print(f\"  ⚠️ Network params excluded (student succeeded)\")\n",
    "\n",
    "# STAGE 3: Not completed (USE network params)\n",
    "not_completed_features = [\n",
    "    'Response Time (sec)',\n",
    "    'RTT (ms)',\n",
    "    'Jitter (ms)',\n",
    "    'Stability (%)',\n",
    "    'Network_Quality_Encoded'\n",
    "]\n",
    "\n",
    "X_not_completed = not_completed[not_completed_features].copy()\n",
    "y_not_completed = not_completed['Engagement_Encoded'].copy()\n",
    "\n",
    "print(\"\\nStage 3 - Not Completed Questions:\")\n",
    "print(f\"  Features: {not_completed_features}\")\n",
    "print(f\"  Shape: {X_not_completed.shape}\")\n",
    "print(f\"  ✅ Network params included for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation with Participant Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge not completed with participant data\n",
    "not_completed_with_tracking = not_completed.merge(\n",
    "    participant_df[participant_df['Event Type'] == 'Joined'][\n",
    "        ['Admission No', 'Quiz#', 'Completion Rate (%)', 'Had Network Issue']\n",
    "    ],\n",
    "    on=['Admission No', 'Quiz#'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Not Completed Questions Analysis:\")\n",
    "print(f\"\\nWith Valid Network Issue:\")\n",
    "valid_network = not_completed_with_tracking[\n",
    "    (not_completed_with_tracking['Completion Rate (%)'] < 70) &\n",
    "    (not_completed_with_tracking['Had Network Issue'] == True)\n",
    "]\n",
    "print(f\"  Count: {len(valid_network)}\")\n",
    "\n",
    "print(f\"\\nLikely Engagement Issue (not network):\")\n",
    "engagement_issue = not_completed_with_tracking[\n",
    "    (not_completed_with_tracking['Completion Rate (%)'] < 70) &\n",
    "    (not_completed_with_tracking['Had Network Issue'] == False)\n",
    "]\n",
    "print(f\"  Count: {len(engagement_issue)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler_initial = StandardScaler()\n",
    "scaler_completed = StandardScaler()\n",
    "scaler_not_completed = StandardScaler()\n",
    "\n",
    "X_initial_scaled = scaler_initial.fit_transform(X_initial)\n",
    "X_completed_scaled = scaler_completed.fit_transform(X_completed)\n",
    "X_not_completed_scaled = scaler_not_completed.fit_transform(X_not_completed)\n",
    "\n",
    "print(\"✅ Features scaled\")\n",
    "print(f\"\\nScaled shapes:\")\n",
    "print(f\"  Initial: {X_initial_scaled.shape}\")\n",
    "print(f\"  Completed: {X_completed_scaled.shape}\")\n",
    "print(f\"  Not Completed: {X_not_completed_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Drive\n",
    "OUTPUT_PATH = '/content/drive/MyDrive/FYP_Data/Preprocessed/'\n",
    "!mkdir -p \"$OUTPUT_PATH\"\n",
    "\n",
    "# Save feature matrices\n",
    "np.save(OUTPUT_PATH + 'X_initial_scaled.npy', X_initial_scaled)\n",
    "np.save(OUTPUT_PATH + 'y_initial.npy', y_initial.values)\n",
    "\n",
    "np.save(OUTPUT_PATH + 'X_completed_scaled.npy', X_completed_scaled)\n",
    "np.save(OUTPUT_PATH + 'y_completed.npy', y_completed.values)\n",
    "\n",
    "np.save(OUTPUT_PATH + 'X_not_completed_scaled.npy', X_not_completed_scaled)\n",
    "np.save(OUTPUT_PATH + 'y_not_completed.npy', y_not_completed.values)\n",
    "\n",
    "# Save scalers\n",
    "import pickle\n",
    "with open(OUTPUT_PATH + 'scaler_initial.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_initial, f)\n",
    "with open(OUTPUT_PATH + 'scaler_completed.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_completed, f)\n",
    "with open(OUTPUT_PATH + 'scaler_not_completed.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_not_completed, f)\n",
    "\n",
    "print(\"✅ All preprocessed data saved to:\")\n",
    "print(OUTPUT_PATH)\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - X_initial_scaled.npy, y_initial.npy\")\n",
    "print(\"  - X_completed_scaled.npy, y_completed.npy\")\n",
    "print(\"  - X_not_completed_scaled.npy, y_not_completed.npy\")\n",
    "print(\"  - scaler_*.pkl (3 files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Preprocessing Complete!**\n",
    "\n",
    "- ✅ Filtered only participating students\n",
    "- ✅ Separated initial questions, completed, and not completed\n",
    "- ✅ Applied proper feature selection:\n",
    "  - Initial: Response Time + Network metrics\n",
    "  - Completed: Response Time + Correctness only\n",
    "  - Not Completed: Response Time + Network metrics\n",
    "- ✅ Cross-validated with participant tracking\n",
    "- ✅ Scaled and saved all data\n",
    "\n",
    "**Next**: Use `02_Model_Training.ipynb` for model training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
