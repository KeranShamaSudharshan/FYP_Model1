{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing with Detailed Step-by-Step Outputs\n",
    "\n",
    "**Fixed Issues**:\n",
    "- ‚úÖ Each student has ONLY 1 initial question (not 3)\n",
    "- ‚úÖ Clusters update dynamically based on performance\n",
    "- ‚úÖ Show dataset state after each transformation\n",
    "- ‚úÖ Balanced data distribution\n",
    "\n",
    "**Dataset**: `Merge_Enhanced_Fixed.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn -q\n",
    "print(\"‚úÖ Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Drive mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Fixed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATA_PATH = '/content/drive/MyDrive/FYP_Data/'\n",
    "df = pd.read_csv(DATA_PATH + 'Merge_Enhanced_Fixed.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*20 + \"STEP 1: DATASET LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal records: {len(df)}\")\n",
    "print(f\"Total students: {df['Admission No'].nunique()}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "\n",
    "print(f\"\\nÔøΩÔøΩ Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(f\"\\nüìä First 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Initial Questions (1 per student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check initial questions\n",
    "initial_q = df[df['Quiz#'] == 0]\n",
    "regular_q = df[df['Quiz#'] > 0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*15 + \"STEP 2: VERIFIED INITIAL QUESTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ Initial Questions (Quiz# = 0):\")\n",
    "print(f\"  Total records: {len(initial_q)}\")\n",
    "print(f\"  Unique students: {initial_q['Admission No'].nunique()}\")\n",
    "\n",
    "# Verify 1 per student\n",
    "initial_per_student = initial_q.groupby('Admission No').size()\n",
    "print(f\"  Questions per student: {initial_per_student.unique()}\")\n",
    "\n",
    "if len(initial_per_student.unique()) == 1 and initial_per_student.unique()[0] == 1:\n",
    "    print(\"  ‚úÖ CORRECT: Each student has exactly 1 initial question\")\n",
    "else:\n",
    "    print(\"  ‚ùå ERROR: Some students have multiple initial questions\")\n",
    "\n",
    "print(f\"\\nüìä Initial Cluster Distribution:\")\n",
    "for cluster, count in initial_q['Engagement Level'].value_counts().items():\n",
    "    print(f\"  {cluster}: {count} ({count/len(initial_q)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Regular Questions (Quiz# > 0):\")\n",
    "print(f\"  Total records: {len(regular_q)}\")\n",
    "\n",
    "print(f\"\\nüìä Sample Initial Questions:\")\n",
    "display(initial_q.head(3)[['Admission No', 'Student Name', 'Question', \n",
    "                           'Response Time (sec)', 'Engagement Level', 'Network Quality']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Filter Participating Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participant tracking\n",
    "participant_df = pd.read_csv(DATA_PATH + 'Participant_Tracking.csv')\n",
    "\n",
    "# Get participating students\n",
    "participated = participant_df[\n",
    "    participant_df['Event Type'] == 'Joined'\n",
    "]['Admission No'].unique()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*15 + \"STEP 3: FILTER PARTICIPATING STUDENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nStudents who joined sessions: {len(participated)}\")\n",
    "print(f\"Total students in dataset: {df['Admission No'].nunique()}\")\n",
    "\n",
    "# Filter\n",
    "df_filtered = df[df['Admission No'].isin(participated)].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ After filtering:\")\n",
    "print(f\"  Records: {len(df_filtered)} (from {len(df)})\")\n",
    "print(f\"  Students: {df_filtered['Admission No'].nunique()}\")\n",
    "\n",
    "if len(participated) == df['Admission No'].nunique():\n",
    "    print(f\"  ‚úÖ All students participated (100% participation rate)\")\n",
    "\n",
    "# Update df\n",
    "df = df_filtered.copy()\n",
    "print(f\"\\nüìä Dataset shape after filtering: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Cluster Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cluster history\n",
    "cluster_history = pd.read_csv(DATA_PATH + 'Cluster_Update_History.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*15 + \"STEP 4: ANALYZE DYNAMIC CLUSTER UPDATES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nStudents with cluster transitions: {len(cluster_history)}\")\n",
    "print(f\"Total cluster transitions: {cluster_history['Transitions'].sum()}\")\n",
    "\n",
    "print(f\"\\nüìä Cluster Transition Examples:\")\n",
    "display(cluster_history.head(10))\n",
    "\n",
    "print(f\"\\nüìä Transition Statistics:\")\n",
    "print(f\"  Students who improved (changed cluster): {len(cluster_history[cluster_history['Transitions'] > 0])}\")\n",
    "print(f\"  Average transitions per student: {cluster_history['Transitions'].mean():.2f}\")\n",
    "print(f\"  Max transitions: {cluster_history['Transitions'].max()}\")\n",
    "\n",
    "# Visualize transitions\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "cluster_history['Transitions'].hist(bins=20, edgecolor='black')\n",
    "plt.xlabel('Number of Transitions')\n",
    "plt.ylabel('Number of Students')\n",
    "plt.title('Distribution of Cluster Transitions')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "transition_matrix = pd.crosstab(cluster_history['Initial Cluster'], \n",
    "                               cluster_history['Final Cluster'])\n",
    "sns.heatmap(transition_matrix, annot=True, fmt='d', cmap='YlOrRd')\n",
    "plt.title('Cluster Transition Matrix')\n",
    "plt.xlabel('Final Cluster')\n",
    "plt.ylabel('Initial Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Cluster updates tracked successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Separate by Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate\n",
    "initial_questions = df[df['Quiz#'] == 0].copy()\n",
    "quiz_questions = df[df['Quiz#'] > 0].copy()\n",
    "\n",
    "# Further separate by completion\n",
    "completed = quiz_questions[quiz_questions['Attempt Status'] == 'Completed'].copy()\n",
    "not_completed = quiz_questions[quiz_questions['Attempt Status'] == 'Not Completed'].copy()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*15 + \"STEP 5: SEPARATE BY QUESTION TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Dataset Breakdown:\")\n",
    "print(f\"  Initial Questions: {len(initial_questions)}\")\n",
    "print(f\"  Quiz Questions: {len(quiz_questions)}\")\n",
    "print(f\"    - Completed: {len(completed)} ({len(completed)/len(quiz_questions)*100:.1f}%)\")\n",
    "print(f\"    - Not Completed: {len(not_completed)} ({len(not_completed)/len(quiz_questions)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Engagement Distribution by Type:\")\n",
    "print(f\"\\nInitial Questions:\")\n",
    "print(initial_questions['Engagement Level'].value_counts())\n",
    "print(f\"\\nCompleted Questions:\")\n",
    "print(completed['Engagement Level'].value_counts())\n",
    "print(f\"\\nNot Completed Questions:\")\n",
    "print(not_completed['Engagement Level'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(data_df):\n",
    "    df_prep = data_df.copy()\n",
    "    \n",
    "    # Binary encoding\n",
    "    df_prep['Is_Correct_Binary'] = df_prep['Is Correct'].apply(\n",
    "        lambda x: 1 if str(x).lower() == 'yes' else 0\n",
    "    )\n",
    "    \n",
    "    # Engagement encoding\n",
    "    engagement_map = {'Passive': 0, 'Moderate': 1, 'Active': 2}\n",
    "    df_prep['Engagement_Encoded'] = df_prep['Engagement Level'].map(engagement_map)\n",
    "    \n",
    "    # Network quality encoding\n",
    "    network_map = {'Poor': 0, 'Fair': 1, 'Good': 2, 'Excellent': 3}\n",
    "    df_prep['Network_Quality_Encoded'] = df_prep['Network Quality'].map(network_map)\n",
    "    df_prep['Network_Quality_Encoded'].fillna(1, inplace=True)\n",
    "    \n",
    "    return df_prep\n",
    "\n",
    "# Apply\n",
    "initial_questions = prepare_features(initial_questions)\n",
    "completed = prepare_features(completed)\n",
    "not_completed = prepare_features(not_completed)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*15 + \"STEP 6: FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ Created binary encodings:\")\n",
    "print(f\"  - Is_Correct_Binary (0/1)\")\n",
    "print(f\"  - Engagement_Encoded (0=Passive, 1=Moderate, 2=Active)\")\n",
    "print(f\"  - Network_Quality_Encoded (0=Poor, 1=Fair, 2=Good, 3=Excellent)\")\n",
    "\n",
    "print(f\"\\nüìä Sample encoded data:\")\n",
    "display(initial_questions[['Engagement Level', 'Engagement_Encoded', \n",
    "                          'Network Quality', 'Network_Quality_Encoded']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Selection by Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" \"*15 + \"STEP 7: FEATURE SELECTION BY STATUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# STAGE 1: Initial (baseline clustering)\n",
    "initial_features = ['Response Time (sec)', 'RTT (ms)', 'Jitter (ms)', 'Stability (%)']\n",
    "X_initial = initial_questions[initial_features].copy()\n",
    "y_initial = initial_questions['Engagement_Encoded'].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Stage 1 - Initial Questions (Baseline):\")\n",
    "print(f\"  Features: {initial_features}\")\n",
    "print(f\"  Shape: {X_initial.shape}\")\n",
    "print(f\"  Rationale: Use all metrics for initial engagement assessment\")\n",
    "\n",
    "# STAGE 2: Completed (NO network params)\n",
    "completed_features = ['Response Time (sec)', 'Is_Correct_Binary']\n",
    "X_completed = completed[completed_features].copy()\n",
    "y_completed = completed['Engagement_Encoded'].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Stage 2 - Completed Questions:\")\n",
    "print(f\"  Features: {completed_features}\")\n",
    "print(f\"  Shape: {X_completed.shape}\")\n",
    "print(f\"  ‚ö†Ô∏è  Network params EXCLUDED (student succeeded)\")\n",
    "print(f\"  Rationale: Network not a factor if question completed successfully\")\n",
    "\n",
    "# STAGE 3: Not Completed (USE network params)\n",
    "not_completed_features = ['Response Time (sec)', 'RTT (ms)', 'Jitter (ms)', \n",
    "                          'Stability (%)', 'Network_Quality_Encoded']\n",
    "X_not_completed = not_completed[not_completed_features].copy()\n",
    "y_not_completed = not_completed['Engagement_Encoded'].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Stage 3 - Not Completed Questions:\")\n",
    "print(f\"  Features: {not_completed_features}\")\n",
    "print(f\"  Shape: {X_not_completed.shape}\")\n",
    "print(f\"  ‚úÖ Network params INCLUDED for validation\")\n",
    "print(f\"  Rationale: Need to check if network caused failure\")\n",
    "\n",
    "print(f\"\\nüìä Feature Statistics:\")\n",
    "print(f\"\\nInitial Questions:\")\n",
    "display(X_initial.describe())\n",
    "print(f\"\\nCompleted Questions:\")\n",
    "display(X_completed.describe())\n",
    "print(f\"\\nNot Completed Questions:\")\n",
    "display(X_not_completed.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scalers\n",
    "scaler_initial = StandardScaler()\n",
    "scaler_completed = StandardScaler()\n",
    "scaler_not_completed = StandardScaler()\n",
    "\n",
    "# Fit and transform\n",
    "X_initial_scaled = scaler_initial.fit_transform(X_initial)\n",
    "X_completed_scaled = scaler_completed.fit_transform(X_completed)\n",
    "X_not_completed_scaled = scaler_not_completed.fit_transform(X_not_completed)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*15 + \"STEP 8: STANDARDIZE FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ Scaled using StandardScaler:\")\n",
    "print(f\"  Initial: {X_initial_scaled.shape}\")\n",
    "print(f\"  Completed: {X_completed_scaled.shape}\")\n",
    "print(f\"  Not Completed: {X_not_completed_scaled.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Scaling Statistics (Initial):\")\n",
    "print(f\"  Mean: {X_initial_scaled.mean(axis=0)}\")\n",
    "print(f\"  Std: {X_initial_scaled.std(axis=0)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All features normalized to mean=0, std=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "OUTPUT_PATH = '/content/drive/MyDrive/FYP_Data/Preprocessed/'\n",
    "!mkdir -p \"$OUTPUT_PATH\"\n",
    "\n",
    "# Save arrays\n",
    "np.save(OUTPUT_PATH + 'X_initial_scaled.npy', X_initial_scaled)\n",
    "np.save(OUTPUT_PATH + 'y_initial.npy', y_initial.values)\n",
    "np.save(OUTPUT_PATH + 'X_completed_scaled.npy', X_completed_scaled)\n",
    "np.save(OUTPUT_PATH + 'y_completed.npy', y_completed.values)\n",
    "np.save(OUTPUT_PATH + 'X_not_completed_scaled.npy', X_not_completed_scaled)\n",
    "np.save(OUTPUT_PATH + 'y_not_completed.npy', y_not_completed.values)\n",
    "\n",
    "# Save scalers\n",
    "import pickle\n",
    "with open(OUTPUT_PATH + 'scaler_initial.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_initial, f)\n",
    "with open(OUTPUT_PATH + 'scaler_completed.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_completed, f)\n",
    "with open(OUTPUT_PATH + 'scaler_not_completed.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_not_completed, f)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*15 + \"STEP 9: SAVE PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved to: {OUTPUT_PATH}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  1. X_initial_scaled.npy ({X_initial_scaled.shape})\")\n",
    "print(f\"  2. y_initial.npy ({len(y_initial)},)\")\n",
    "print(f\"  3. X_completed_scaled.npy ({X_completed_scaled.shape})\")\n",
    "print(f\"  4. y_completed.npy ({len(y_completed)},)\")\n",
    "print(f\"  5. X_not_completed_scaled.npy ({X_not_completed_scaled.shape})\")\n",
    "print(f\"  6. y_not_completed.npy ({len(y_not_completed)},)\")\n",
    "print(f\"  7. scaler_initial.pkl\")\n",
    "print(f\"  8. scaler_completed.pkl\")\n",
    "print(f\"  9. scaler_not_completed.pkl\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ All data preprocessed and saved\")\n",
    "print(f\"‚úÖ Ready for model training\")\n",
    "print(f\"\\nNext: Run 02_Model1_Clustering_Prediction.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
